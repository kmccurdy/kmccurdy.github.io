<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://kmccurdy.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kmccurdy.github.io/" rel="alternate" type="text/html" /><updated>2021-11-03T13:22:45+00:00</updated><id>https://kmccurdy.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">neural models differ from speakers in how they inflect new words</title><link href="https://kmccurdy.github.io/blog/2020/acl2020-summary/" rel="alternate" type="text/html" title="neural models differ from speakers in how they inflect new words" /><published>2020-07-06T00:00:00+00:00</published><updated>2020-07-06T00:00:00+00:00</updated><id>https://kmccurdy.github.io/blog/2020/acl2020-summary</id><content type="html" xml:base="https://kmccurdy.github.io/blog/2020/acl2020-summary/">&lt;p&gt;for ACL 2020, I wrote up a nontechnical summary of our research. posting here in case it’s useful.&lt;/p&gt;

&lt;h1 id=&quot;inflecting-when-theres-no-majority-limitations-of-encoder-decoders-as-cognitive-models-of-german-plurals&quot;&gt;Inflecting When There’s No Majority: Limitations of Encoder-Decoders as Cognitive Models of German Plurals&lt;/h1&gt;

&lt;p&gt;Kate McCurdy, Sharon Goldwater, Adam Lopez&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.159&quot;&gt;Our paper&lt;/a&gt; looks at whether artificial neural networks behave similarly to human speakers when faced with new words. Neural networks successfully learn to extend frequent patterns to new words. For example, they can learn to use the English plural suffix -s (as in &lt;em&gt;dogs&lt;/em&gt; and &lt;em&gt;cats&lt;/em&gt;) for new words (e.g. &lt;em&gt;wugs&lt;/em&gt;), and some researchers have argued that this behavior is human-like. In some cases, however, human speakers also extend infrequent or rare patterns to new words. A hypothetical example in English would be a new word which took the same singular and plural form, like &lt;em&gt;sheep&lt;/em&gt; or &lt;em&gt;deer&lt;/em&gt;; however, while this almost never occurs in English, there are other languages where speakers reliably apply rare patterns to new words. Can neural models also learn this behavior?&lt;/p&gt;

&lt;p&gt;If a German speaker learns a new word &lt;em&gt;Bral&lt;/em&gt; and wants to produce its plural form, that speaker must make a decision. Will &lt;em&gt;Bral&lt;/em&gt; take the suffix -e (like the existing word &lt;em&gt;Male&lt;/em&gt;, which means “times”) or -en (like &lt;em&gt;Wahlen&lt;/em&gt;, “votes”), both of which are quite frequent patterns in German? Or will it take a more rare suffix such as -er (like &lt;em&gt;Täler&lt;/em&gt;, “valleys”) or -s (like &lt;em&gt;Schals&lt;/em&gt;, “scarves”)? Based on previous linguistic research, the speaker is more likely to use the rare suffix -s if the new word sounds more unusual (e.g. &lt;em&gt;Bnöhk&lt;/em&gt; or &lt;em&gt;Plaupf&lt;/em&gt;), rather than similar to existing German words (e.g. &lt;em&gt;Bral&lt;/em&gt;). This feature of the German plural system makes it a useful test case to evaluate whether neural models can learn to generalize rare patterns as speakers do.&lt;/p&gt;

&lt;p&gt;Our experiment used two lists of made-up words, developed by previous researchers: Rhymes, which sound like existing German words (e.g. &lt;em&gt;Bral&lt;/em&gt;, &lt;em&gt;Spert&lt;/em&gt;), and Non-Rhymes, which sound more unusual (e.g. &lt;em&gt;Bnöhk&lt;/em&gt;, &lt;em&gt;Plaupf&lt;/em&gt;). We presented these words to German speakers and asked them to produce the plural forms of these words. We also trained a neural network to produce plural forms of German words, and presented the same list of made-up words to the network.&lt;/p&gt;

&lt;p&gt;Our results indicate that neural models do &lt;em&gt;not&lt;/em&gt; use rare patterns the way speakers do. We found that speakers used the rare suffix -s more on Non-Rhymes compared to Rhymes, consistent with earlier studies. By contrast, the neural network did not use -s more on Non-Rhymes; instead, it showed a &lt;em&gt;frequency bias&lt;/em&gt;, using the frequent suffix -e more on Non-Rhymes than Rhymes. This finding suggests that neural models do not fully capture human speaker cognition: they fail to learn the conditions under which speakers generalize rare patterns.&lt;/p&gt;</content><author><name></name></author><summary type="html">for ACL 2020, I wrote up a nontechnical summary of our research. posting here in case it’s useful.</summary></entry></feed>